{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MilicaMedic/TPQ_final/blob/main/Deep_Reinforcement_Learning_for_the_Multiple_Cryptocurrency_Trading_Using_an_Ensemble_Strategy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXaoZs2lh1hi"
      },
      "source": [
        "# Deep Reinforcement Learning for the Multiple Cryptocurrency Trading Using an Ensemble Strategy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sApkDlD9LIZv"
      },
      "source": [
        "# Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjLD2TZSLKZ-"
      },
      "source": [
        "The emergence of cryptocurrencies has disrupted traditional financial systems by opening various new investment opportunities. The objective of this project is to implement an automated solution for trading multiple cryptocurrencies by using the Ensembled trading strategy that consist of the following Deep Reinforcement Learning (DRL) agents: Proximal Policy Optimization (PPO), Advantage Actor-Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The Ensembled strategy combines the best features of the three algorithms, aiming to create a robust solution for adjusting to different market conditions, with the goal to maximize the profits (returns).\n",
        "The project trains, and trades, on the same dataset, all three agents, the Ensemble strategy, and the min-variance as baseline, and gives a performance comparison of all strategies.\n",
        "\n",
        "All three agents in an Ensemble stragegy are trained at the same time, quarterly (3-month period), and after each time the best agent is selected based on their Sharpe ratio and used for trading in the next quarter.\n",
        "\n",
        "For this project the FinRL, an open source framework for financial reinforcement learning, is used.\n",
        "This work includes 5 crypto tokens that have adequate liquidity, market capitalisation and, also, to how long the coin is listed (how long the data exists, so that it went through a couple of bull/bear cycles).\n",
        "\n",
        "The cryptocurrency market is challenging due to its high volatility, and unpredictability, offering at the same time opportunities for the significant profits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffsre789LY08"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woNruzYwjLYu"
      },
      "source": [
        "In the last decade Reinforcement Leaning (RL) has demonstrated promising results in the finance field. RL enables an agent to learn optimal trading policies through continuous interaction with its environment (e.g. finance market data), allowing for dynamic and efficient decision-making. In this context, the agent uses algorithms to learn the optimal policy without prior knowledge of the state transition probabilities or reward functions, relying instead on observed experiences. RL can be applied as follows:\n",
        "\n",
        "* Agent: The decision-making algorithm that executes trades.\n",
        "* Environment: A simulated or real-world representation of the market, including asset prices, technical indicators, volatility, sentiment, etc.\n",
        "* State: The information the agent uses to assess the current market conditions.\n",
        "* Action: The possible trading operations, such as buy, sell, or hold.\n",
        "* Reward: A metric that quantifies the agent’s performance, often tied to risk-adjusted returns, or other financial objectives. The primary goal of RL in trading is to train the agent to maximize long-term rewards while effectively managing market risks.\n",
        "  \n",
        "One of the latest advancements in the field of Machine Learning and Artificial Intelligence is Deep Reinforcement Learning (DRL). Deep Learning refers to all algorithms that use deep neural networks (neural networks with at least one hidden layer).\n",
        "\n",
        "DRL combines Deep Neural Networks with Reinforcement Learning to manage complex and high-dimensional data. This is particularly relevant in cryptocurrency trading, where numerous factors such as price movements, technical indicators, and broader economic influences (news, tweets), whales behaviour, etc., interact to create complex market dynamics. By using neural networks, DRL models these relationships, enabling the agents to develop an advanced and adaptable trading strategies.\n",
        "\n",
        "The application DRL provides a powerful framework for addressing the challenges of cryptocurrency trading. These techniques allow for dynamic adaptation to changing market conditions, development of sophisticated strategies, and efficient risk management. Leveraging these algorithms enables traders and researchers to create robust, scalable solutions capable of thriving in the unpredictable cryptocurrency market."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGv01K8Sh1hn"
      },
      "source": [
        "## Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPT0ipYE28wL"
      },
      "outputs": [],
      "source": [
        "# ## install finrl library\n",
        "#!pip3 install wrds\n",
        "#!pip3 install swig\n",
        "#!pip3 install empyrical\n",
        "#!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeMK7Uentj1V"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iSlL3vYjLYy"
      },
      "source": [
        "For the purpose of this project the finrl file models.py (finrl/agents/stablebaselines3/models) is changed into a2c_ppo_ddpg.py to modify the DRLAgent, DRLEnsembleAgent classes, and the run_ensemble_strategy function for the purpose of using only selected agents. Also, the StockTradingEnv class is modified to allow bying fraction of the cryptocurrency coin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPqeTTwoh1hn",
        "outputId": "f3e755dc-7711-4131-ab90-3974019aaf44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas_market_calendars'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-4106587293.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureEngineer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_processor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_stock_trading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_stocktrading\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStockTradingEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/finrl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrade\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrade\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/finrl/trade.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_stock_trading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_stock_papertrading\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlpacaPaperTrading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/finrl/meta/env_stock_trading/env_stock_papertrading.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_processors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor_alpaca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlpacaProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/finrl/meta/data_processors/processor_alpaca.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas_market_calendars\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malpaca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStockHistoricalDataClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas_market_calendars'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from empyrical import max_drawdown, cum_returns, sharpe_ratio, annual_return, annual_volatility\n",
        "import datetime\n",
        "%matplotlib inline\n",
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3 import a2c_ppo_ddpg\n",
        "from finrl.agents.stablebaselines3.a2c_ppo_ddpg import DRLAgent, DRLEnsembleAgent\n",
        "from stable_baselines3 import PPO, A2C, DDPG\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
        "from pprint import pprint\n",
        "import sys\n",
        "sys.path.append(\"../FinRL\")\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9A8CN5R5PuZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import (\n",
        "    DATA_SAVE_DIR,\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR,\n",
        "    INDICATORS,\n",
        ")\n",
        "\n",
        "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A289rQWMh1hq"
      },
      "source": [
        "# Getting the data\n",
        "\n",
        "* The dataset, used as a crypto environment, consists of the daily OHLCV data for the period from 01/01/2021 to 03/31/2023., for the 5 largest cryptocurrencies by market capitalisation at the moment of obtaining the data.\n",
        "\n",
        "* Data is obtained from the Yahoo Finance API, and stored into the data.csv file. Code used for getting the data can be found in the YahooDownloader.py file. Yahoo Finance API is a free provider of stock and crypto data, financial news, financial reports, etc.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCKm4om-s9kE",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('data.csv')\n",
        "data['date'] = pd.to_datetime(data['date'])\n",
        "data = data.sort_values(['date','tic'])\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETKoi5u1jLY0"
      },
      "outputs": [],
      "source": [
        "data.tic.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqC6c40Zh1iH"
      },
      "source": [
        "# Preparing Data\n",
        "\n",
        "* The price trend and momentum information is extracted from the dataset, and preserved by the technical indicators\n",
        "* The FinRL framework has the FeatureEngineer class that enables adding a list of the technical indicators listed below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8OV3FgSjLY1"
      },
      "outputs": [],
      "source": [
        "INDICATORS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1whFtGV5jLY1"
      },
      "source": [
        "**Moving Average Convergence Divergence (MACD):** A trend-following momentum indicator that shows the relationship between two exponential moving averages (EMAs) of an asset’s price. It helps detect trend direction, strength, and potential reversals, making it a valuable feature in crypto trading and reinforcement learning (RL) state spaces. It consists of three Exponential Moving Averages (EMA), most often 26, 12, and 9 (used in this project) periods. MACD is calculated by subtracting the long (26) EMA from short EMA (12), and having the 9 period EMA as a signal line. In the most simplest usage of MACD, when a MACD line falls below the signal line, it may be signal to sell, as it indicates a bearish momentum. Same way, if a MACD line is above the signal line, it is a signal to sell, as it indicates a bullish momentum.\n",
        "\n",
        "**Relative Strength Index (RSI):** A momentum oscillator used in financial markets to measure the speed and change of price movements. It is widely used in technical analysis to identify overbought or oversold assets. RSI oscillator moves between values 0 and 100. If the RSI value gets above 70, indicates an overbought asset, and may indicate a price reversal. A RSI index below 30 indicates than an asset is oversold, and may indicate a price bounc. Likewise, if a signal value is above 50, it indicates a bullish momentum, and below 50 indicates a bearish momentum.\n",
        "\n",
        "**Directional Index (DX):** An indicator to determine the strength of a trend\n",
        "\n",
        "**Commodity Channel Index (CCI):** An oscillator that indicates an overbought, or an oversold asset, and detecting possible new trends.\n",
        "\n",
        "**Bollinger Bands (BOLL):** An indicator used to measure a market’s volatility, and identify overbought, or oversold assets. BOLL are calculated by using a Moving Average (MA) (usually 20 periods), and subtracting, and adding 2x Standard Deviation (STD) to get the lower band (boll_lb), and upper band (boll_ub) respectively.\n",
        "\n",
        "**Simple Moving Average (SMA):** Shows an average price of an asset across the selected time period. SMA is, usually, calculated by using the closing prices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCiQz8QKjLY2"
      },
      "source": [
        "The FinRL framework, also, offers adding the financial turbulence index which measures the price fluctuations, and is used to control trading in the moments of high volatility, and controlling the risks in the cases of the extreme events. The Turbulence threshold is used in a way that, if the current turbulence value is higher, or equal to the threshold, all positions of the asset will be cleared.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgXfBcjxtj1a"
      },
      "outputs": [],
      "source": [
        "feature_engineer = FeatureEngineer(use_technical_indicator=True,\n",
        "                     tech_indicator_list = INDICATORS,\n",
        "                     use_turbulence=True,\n",
        "                     user_defined_feature = False)\n",
        "\n",
        "data = feature_engineer.preprocess_data(data)\n",
        "data = data.copy()\n",
        "data = data.fillna(0)\n",
        "data = data.replace(np.inf,0)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DbEjL5mjLY3"
      },
      "outputs": [],
      "source": [
        "# Plotting and calculating the maximum turbulence\n",
        "%matplotlib inline\n",
        "print(data[['turbulence']].max())\n",
        "data['turbulence'].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96oFPkMhjLY3"
      },
      "source": [
        "Splitting data into training and trading set, by using the FinRL built in *data_split* function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1ZFjlhOjLY3"
      },
      "outputs": [],
      "source": [
        "TRAIN_START_DATE = \"2021-01-01\"\n",
        "TRAIN_END_DATE = \"2024-03-01\"\n",
        "TRADE_START_DATE = \"2024-05-05\"\n",
        "TRADE_END_DATE = \"2025-03-15\"\n",
        "\n",
        "#Creating train and trade datasets\n",
        "train_data = data_split(data, TRAIN_START_DATE,TRAIN_END_DATE)\n",
        "trade_data = data_split(data, TRADE_START_DATE,TRADE_END_DATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QsYaY0Dh1iw"
      },
      "source": [
        "# Environment\n",
        "\n",
        "The trading task is inherently stochastic and is modeled as a Markov Decision Process (MDP), aiming to maximize returns over time. An agent learns through interactions with the market—observing states, executing actions, and receiving rewards that guide policy updates.\n",
        "\n",
        "By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
        "\n",
        "**Action space:** The action space defines all the possible actions an agent can take at each time step. In a multi-asset portfolio, this involves choosing how to allocate capital across several cryptocurrencies. coin).\n",
        "\n",
        "Multi-asset portfolio uses a buy/sell signals translated into position weights for every coin. Each element corresponds to an allocation weight or action (e.g., proportion of capital allocated to each coin). For example, \"Buy 10 coins of ETC\" or \"Sell 10 coins of ETC\" are 10 or -10, respectively. The agent outputs a vector of real numbers, normalized to a range of [-1, 1], because the policy for the A2C, and PPO agents is defined as a Gaussian distribution.\n",
        "\n",
        "**Reward function:** The reward function guides the agent toward maximizing some measure of portfolio performance. In trading, the reward is used to incentivize profitable trades, penalize risk or volatility, and reflect realistic trading constraints (e.g., slippage, transaction costs). The reward is calculated as a change of the portfolio value when action $a$ is taken at state $s$, and arriving at new state $s'$, by the following  $r(s, a, s′) = v′ − v$, where $v′$ and $v$ represent the portfolio values at state $s'$ and $s$, respectively. Reward is then scaled by reward_scale = 1e-4, but tested also with scaling of self.reward_scale = 1e-12, and reward_scale = 1e-8. Best results are obtained with the 1e-4 scaling, in combination with hyperparameters.\n",
        "\n",
        "**State space:** In a DRL trading environment, the state space defines the information available to the agent at each time step to make trading decisions. It is essentially an observation or an input vector that the neural networks (actor and critic) consume to determine the best action. The state space of this problem contains 51 dimensions, including:\n",
        "- Available balance at the every timestep\n",
        "- Current prices of 5 crypto assets\n",
        "- 8 indicators listed abowe\n",
        "- Current Action Space, which is a 5-dimensional continuous action space, each one represents the number of each asset to be purchased or to be sold.\n",
        "\n",
        "**Market environment data:** OHLCV data for 5 cryptocurrencies, plus 8 indicators\n",
        "\n",
        "**Trading frequency:** Daily\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2zqII8rMIqn"
      },
      "outputs": [],
      "source": [
        "asset_dimension = len(data['tic'].unique())\n",
        "state_space = 1 + 2*asset_dimension + len(INDICATORS)*asset_dimension\n",
        "print(f\"Asset Dimension: {asset_dimension}, State Space: {state_space}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02zalgR5jLY5"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWyp84Ltto19"
      },
      "outputs": [],
      "source": [
        "buy_cost_list = sell_cost_list = [0] * asset_dimension\n",
        "num_stock_shares = [0] * asset_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100, #maximum number of coins to trade\n",
        "    \"initial_amount\": 1000000, #initial capital\n",
        "    \"num_stock_shares\": num_stock_shares, #number of assets\n",
        "    \"buy_cost_pct\": buy_cost_list, #transaction cost percentage per trade, not used for the sake od simplicity\n",
        "    \"sell_cost_pct\": buy_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": asset_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS, #list of techinal indicators\n",
        "    \"action_space\": asset_dimension,\n",
        "    \"reward_scaling\": 1e-4, #scaling factor for reward, good for training\n",
        "    \"print_verbosity\":5\n",
        "\n",
        "}\n",
        "\n",
        "e_train_gym = StockTradingEnv(df = train_data, **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZFMz7aWjLY5"
      },
      "outputs": [],
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "e_trade_gym = StockTradingEnv(df = trade_data, turbulence_threshold = 700, **env_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "# DRL Algorithms\n",
        "\n",
        "Parallel testing of the Ensemble strategy combining A2C, PPO, and DDPG agents, and A2C, PPO, and TD3 agents, resulted with the TD3 agent performing superior, as a standalone model, and DDPG usually having the worst standalone performance. However, during the simulations of the Ensemble strategy, using the TD3 algorithm instead of the DDPG model, did not improve performance of the Ensemble strategy. In some cases, when the TD3 model is used the Ensemble strategy used mostly the A2C and PPO models, which showed that it is better to use the DDPG algorithm. In a different timeframes for learning and trading the PPO model significantly overperformed all other strategies, almost perfectly foolowing trends. However, it showed that the dataset also impacts the models performances, and therefore the Ensemble strategy.\n",
        "\n",
        "Therefore, the Ensemble strategy is a good strategy to combine the three models utilising the best properties of each DRL algorithm. As the strategy is combining A2C, PPO, and DDPG with the goal of selecting the best-performing agent each quarter based on Sharpe ratio, as part of an ensemble trading strategy. Given that ensemble stability and adaptability are more important than individual agent performance, the accent was on setting up the parameters to achieve a complementary behavior across the agents and a robust regime coverage, above the isolated agent success.\n",
        "\n",
        "The A2C, PPO, and DDPG agents all implement the fundamental RL loop of interacting with an environment, receiving rewards, and learning from the experience, while using different strategies for balancing learning stability, sample efficiency, and action flexibility.\n",
        "The A2C and the PPO agents use the stochastic policies and an advantage estimation, while the DDPG agent uses the deterministic policies. Each DRL algorithm has a distinct mechanism for updating its value function and policy, with the PPO emphasizing the stable updates, but all three agents rely on the environment to provide rewards and feedback that drive learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROiJmL_0jLY6"
      },
      "source": [
        "## Setting the hyperparameters for training and trading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv0Uu7B2jLY6"
      },
      "source": [
        "During the testing of the project, the biggest challege was setting up the optimal hyperparameters for the DRL models, as their performance is heavily dependant on them, and required a lot of tuning, which is not straightforward. Also, there is a difference between the DRL model in learning efficiences, policy updates, etc. For example, DDPG is a Q- learning based “off-policy” method and learns from past samples using the experience replay buffers.\n",
        "Furthermore, the unpredictable behaviour of the DRL models using the same data and same hyperparameters, made them sometimes overperform the ensemble strategy, but in other cases with the same setting (same parameters and environment) have suboptimal outcome. In the case of the A2C algorithm this is, most likely, because of the liability of the A2C algorithm toward overfitting, which would make it a bad choice to use alone in the real-world trading.\n",
        "\n",
        "During the testing and trading there are two core behaviors a DRL agent must constantly balance well, as it is crucial for learning effective trading policies, especially in complex, noisy environments like crypto markets: exploration and exploitation.\n",
        "\n",
        "During the exploration the agent tries new, or uncertain, actions to learn more about the environment, even if they may not immediately seem profitable, with the goal of finding the potentially better strategies or unexpected profitable actions.\n",
        "During the exploitation it uses its current knowledge of profitable actions to choose the action it believes will yield the highest reward, with the goal to maximize rewards, based on what the agent has already learned.\n",
        "\n",
        "**n_steps:**\n",
        "\n",
        "n_steps is the number of timesteps the DRL agent collects before each update of the policy and value networks. The impact on trading depends on the DRL model. For example, higher values for the A2C model(e.g., 20-50) means that an agent collects more market information before updating, which leads to better stability, but slower agent reactions. Lower values (e.g., 5-10) leads to making the updates more frequently, quicker agent adaptation to the environment but risks increasing the variance in the learning process.\n",
        "\n",
        "**Learning Rate (learning_rate):**\n",
        "\n",
        "Learning rate is a parameter in DRL models that controls how much the model updates its weights during training. If the learning rate is too high the agent learns too quickly and faster adapts, but it can lead to unstable updates and erratic trading decisions. If the learning rate is too low the agent learns slowly but the learning is stable, with the risk that an agent misses profitable trades failing to adapt to new market conditions.\n",
        "\n",
        "**Entropy Coefficient (ent_coef):**\n",
        "\n",
        "Entrophy coefficient controls the level of exploration by adding randomness to actions.\n",
        "High entropy coefficient encourages more exploration, which may be useful in unpredictable markets. However, while entropy is essential early on, high entropy may block the agent from shifting to a confident, profitable policy which may be a reason for the agents underperformance. Low entropy focuses on learned profitable trades.\n",
        "In a noisy, fast-changing environment like crypto, too high entropy coeficient causes an agent to behave randomly, fail to converge, and miss profitable trading opportunities. This can also affect the Ensemble strategy because if the agents are too exploratory, none of them will dominate, which could make the agent selection unreliable. The DDPG agent doesn’t use entropy.\n",
        "\n",
        "**Training Timesteps:**\n",
        "\n",
        "Timesteps are the total number of iterations the DRL model trains. If number of timesteps is low the model doesn’t learn enough to make good trading decisions. This parameter has most impact on the A2C model, but also has a significant influence on the other two models. If the number of timesteps is too high there is a risk that the model will overfit to the historical market conditions.\n",
        "\n",
        "**Batch Size (batch_size):**\n",
        "\n",
        "Batch size refers to a number of the training samples used in one update of the model’s parameters.\n",
        "\n",
        "**For future research:**\n",
        "\n",
        "An optimal solution for hyperparameters setup would be the constant adaptation depending on the market conditions. This is out of the scope for this project, but for the real-world trading it would be something to work on before deploying the strategy to trade on the exchange. For example, adapting the learning rate by reducing it when the model stabilizes, and increasing when retraining in new markets. Another example would be to gradually reduce enthropy coefficient as the model becomes more confident in its strategy. Increasing the number of timesteps would also have a big impact on the performace of the agents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68g9CZIPjLY6"
      },
      "outputs": [],
      "source": [
        "A2C_model_kwargs = {\n",
        "                    'n_steps': 5,\n",
        "                    'ent_coef': 0.001,\n",
        "                    'learning_rate': 0.0007\n",
        "                    }\n",
        "\n",
        "PPO_model_kwargs = {\n",
        "                    \"ent_coef\": 0.001,\n",
        "                    \"n_steps\": 2048,\n",
        "                    \"learning_rate\": 0.0005,\n",
        "                    \"batch_size\": 128\n",
        "                    }\n",
        "\n",
        "\n",
        "DDPG_model_kwargs = {\n",
        "                     \"buffer_size\": 10000,\n",
        "                     \"learning_rate\": 0.0005,\n",
        "                     \"batch_size\": 128\n",
        "                    }\n",
        "\n",
        "\n",
        "timesteps_dict = {\n",
        "                 'a2c': 100000,\n",
        "                 'ppo': 100000,\n",
        "                 'ddpg': 100000\n",
        "                 }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAs7RyyljLY7"
      },
      "source": [
        "###  The Advantage Actor-Critic (A2C) model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFP3M6gujLY7"
      },
      "source": [
        "The Advantage Actor-Critic (A2C) algorithm is a widely used method within the family of policy-gradient reinforcement learning algorithms. The A2C model serves as a synchronous version of the older asynchronous variant known as the A3C (Asynchronous Advantage Actor-Critic) algorithm, and brings improvement in model stability by updating the parameters synchonously, after collecting a batch of experiences, rather than asynchronously.\n",
        "\n",
        "The goal of the A2C model is to maximize the expected cumulative reward an agent can achieve over time through the interaction with the environment. To achieve this, the A2C model uses two neural networks (actor and critic) that separately address the policy optimization (via the actor), and the value estimation (via the critic), and collaborate with a goal of learning the optimal policy:\n",
        "\n",
        "**Actor (Policy Network):** The actor network, in a time $t$, as an input has the current state $s_t$, and outputs an action $a_t$. The actor network learns the policy function $\\pi(a \\mid s)$, which maps the input state to a probability distribution over all possible actions. Based on these probabilities the actor chooses the action which is then applied to the environment, while the environment returns the next state $s_{t+1}$ and a reward.\n",
        "\n",
        "**Critic (Value Network):** The critic network learns the value function $V(s_t)$ which estimates the expected rewards starting from a state $s$, under the current policy.\n",
        "\n",
        "The Advantage function is calculated as a difference between the actual reward and the expected reward:\n",
        "$$\n",
        " A(s_t, a_t) = r_t + \\gamma V(s_{t+1}) - V(s_t)\n",
        "$$\n",
        "\n",
        "where $r_t$ is the immediate reward received after taking action $a_t$, $\\gamma \\in [0, 1]$ is the discount factor, a real number between 0 and 1, which determines the present value of future rewards. In cryprocyrrency tranding $\\gamma$ is 0. The $V(s_{t+1})$ is the estimated value of the next state.\n",
        "\n",
        "This advantage is used to update the policy by adjusting the actor’s parameters to increase the probability of advantageous actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRIbgyb1jLY8"
      },
      "outputs": [],
      "source": [
        "agent = DRLAgent(env = env_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdUsdZW_jLY8"
      },
      "source": [
        "As per documentation, the DRLAgent class provides implementations for DRL algorithms:\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        env: gym environment class\n",
        "            user-defined class\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "        get_model()\n",
        "            setup DRL algorithms\n",
        "        train_model()\n",
        "            train DRL algorithms in a train dataset\n",
        "            and output the trained model\n",
        "        DRL_prediction()\n",
        "            make a prediction in a test dataset and get results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTTgQRxZjLY8"
      },
      "outputs": [],
      "source": [
        "# The get_model function sets the DRL algorithm\n",
        "model_a2c = agent.get_model(\"a2c\", model_kwargs = A2C_model_kwargs)\n",
        "\n",
        "# Set up logger\n",
        "tmp_path = RESULTS_DIR + '/a2c'\n",
        "new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "\n",
        "# Set new logger\n",
        "model_a2c.set_logger(new_logger_a2c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W2umW0CjLY8"
      },
      "outputs": [],
      "source": [
        "# The train_model function takes the selected model, and trains it for n timesteps\n",
        "trained_a2c = agent.train_model (model = model_a2c, tb_log_name = 'a2c', total_timesteps = timesteps_dict['a2c'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIz1M_A1jLY9"
      },
      "outputs": [],
      "source": [
        "# Getting the account value, and the actions DataFrames\n",
        "df_account_value_a2c, df_a2c_actions = DRLAgent.DRL_prediction(model=trained_a2c, environment = e_trade_gym)\n",
        "df_a2c_actions.to_csv('df_a2c_actions.csv')\n",
        "print(df_account_value_a2c)\n",
        "print(df_a2c_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv5QuOvWjLY-"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame with portfolio values in $\n",
        "df_account_value_a2c = pd.DataFrame({'date': df_account_value_a2c['date'], 'account_value_a2c': df_account_value_a2c['account_value']})\n",
        "df_account_value_a2c = df_account_value_a2c.set_index('date')\n",
        "df_account_value_a2c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VA3HNc8jLY-"
      },
      "outputs": [],
      "source": [
        "# Calculate daily returns\n",
        "df_account_value_a2c['daily_return_a2c'] = df_account_value_a2c['account_value_a2c'].pct_change()\n",
        "\n",
        "# Calculate cumluative returns\n",
        "df_account_value_a2c['cumuluative_return_a2c'] = np.exp(np.log1p(df_account_value_a2c['daily_return_a2c']).cumsum())-1\n",
        "df_account_value_a2c['cumuluative_return_a2c']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZIjPHmzjLY_"
      },
      "source": [
        "### The Proximal Policy Optimization (PPO) model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgv0WBprjLY_"
      },
      "source": [
        "The Proximal Policy Optimization (PPO) model is, as well as the A2C model, from the policy-gradient reinforcement learning algorithms, and is based on the similar idea of updating the policy function but, in adittion, the PPO model, uses a clipping mechanism for updating the policy function, and ensures more stable learning compared to the A2C model. The motivation behind the clipping machanism is to optimize the objective function while ensuring the deviation from the previous policy is relatively small, which helps the PPO model to learn faster as it avoids overshooting and needing to relearn bad trades.\n",
        "\n",
        "The PPO loss function is defined as:\n",
        "$$\n",
        "\\text{Loss} = \\min\\left( r(\\theta) A, \\; \\text{clip}(r(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\cdot A \\right)\n",
        "$$\n",
        "where $r(\\theta) = \\frac{\\pi_{\\theta}(a \\mid s)}{\\pi_{\\text{old}}(a \\mid s)} $,  $A$  is the advantage estimate, and $\\epsilon$ is a small numeric value, for example  0.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGauNjcdjLZA"
      },
      "outputs": [],
      "source": [
        "# The get_model function sets the DRL algorithm\n",
        "model_ppo = agent.get_model(\"ppo\", model_kwargs = PPO_model_kwargs)\n",
        "tmp_path = RESULTS_DIR + '/ppo'\n",
        "new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "# Set new logger\n",
        "model_ppo.set_logger(new_logger_ppo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_v90rWFjLZA"
      },
      "outputs": [],
      "source": [
        "trained_ppo = agent.train_model(model = model_ppo, tb_log_name = 'ppo', total_timesteps = timesteps_dict['ppo'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxGMit4GjLZB"
      },
      "outputs": [],
      "source": [
        "# Getting the account value, and the actions DataFrames\n",
        "df_account_value_ppo, df_ppo_actions = DRLAgent.DRL_prediction(model = trained_ppo, environment = e_trade_gym)\n",
        "print(df_account_value_ppo)\n",
        "print(df_ppo_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9Q3NOPQjLZB"
      },
      "source": [
        "The action DataFrame for the PPO model implies tha the PPO model has the most dynamic, and therefore riskier, trading style than the A2C, and the DDPG model which is the most conservative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izi9h3HejLZB"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame with portfolio values in $\n",
        "df_account_value_ppo = pd.DataFrame({'date': df_account_value_ppo['date'],'account_value_ppo': df_account_value_ppo['account_value']})\n",
        "df_account_value_ppo = df_account_value_ppo.set_index('date')\n",
        "df_account_value_ppo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgCa1aCHjLZC"
      },
      "outputs": [],
      "source": [
        "# Calculate daily returns\n",
        "df_account_value_ppo['daily_return_ppo'] = df_account_value_ppo['account_value_ppo'].pct_change()\n",
        "\n",
        "# Calculate cumluative returns\n",
        "df_account_value_ppo['cumuluative_return_ppo'] = np.exp(np.log1p(df_account_value_ppo['daily_return_ppo']).cumsum())-1\n",
        "df_account_value_ppo['cumuluative_return_ppo']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBjQmT4tjLZC"
      },
      "source": [
        "## The Deep Deterministic Policy Gradient (DDPG) model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DvXENjGjLZD"
      },
      "source": [
        "The DDPG model is an off-policy, actor-critic, algorithm specifically designed for environments with continuous action spaces. The DDPG model combines the strengths of Deep Q-Networks (DQN) and Deterministic Policy Gradient (DPG) to learn efficient policies in high-dimensional, continuous domains, such as algorithmic trading or robotic control.\n",
        "\n",
        "The Actor learns a deterministic policy $\\mu(s \\mid \\theta^\\mu)$ , that maps a state $s$ directly to a specific action $a$.\n",
        "The Critic approximates the $Q$-function $Q(s, a \\mid \\theta^Q)$, which estimates the expected cumulative reward for taking an action $a$ in a state $s$, following the current policy. Unlike the stochastic on-policy methods like A2C and PPO, the DDPG model uses a deterministic policy.\n",
        "\n",
        "The DDPG model has the following key elements:\n",
        "\n",
        "A Replay Buffer is a memory buffer that stores the touple of transitions ($s_t$, $a_t$, $r_t$, and $s_t+1$) which are randomly sampled during training. This breaks a correlation between the consecutive samples and stabilizes learning.\n",
        "The Target Networks in the DDPG model maintain the delayed copies of the actor and critic networks, denoted $\\mu$, $\\mu'$ and $Q$, $Q'$.\n",
        "\n",
        "Since the DDPG policy is deterministic, the model introduces an exploration noise, often using the Ornstein–Uhlenbeck (OU) process or an Gaussian noise, added to the actor’s output during training:\n",
        "$$\n",
        "a_t = \\mu(s_t \\mid \\theta^\\mu) + \\mathcal{N}_t\n",
        "$$\n",
        "This method encourages exploration of the state-action space, which is critical in a sparse or noisy reward environments like cryptocurrency trading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BbnAoezjLZD"
      },
      "outputs": [],
      "source": [
        "# The get_model function sets the DRL algorithm\n",
        "model_ddpg = agent.get_model(\"ddpg\", model_kwargs = DDPG_model_kwargs)\n",
        "\n",
        "tmp_path = RESULTS_DIR + '/ddpg'\n",
        "new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "# Set new logger\n",
        "model_ddpg.set_logger(new_logger_ddpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4govrOTjLZD"
      },
      "outputs": [],
      "source": [
        "trained_ddpg = agent.train_model(model = model_ddpg, tb_log_name = 'ddpg', total_timesteps = timesteps_dict['ddpg'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZPSziIYjLZE"
      },
      "outputs": [],
      "source": [
        "# Getting the account value, and the actions DataFrames\n",
        "df_account_value_ddpg, df_ddpg_actions = DRLAgent.DRL_prediction(model = trained_ddpg, environment = e_trade_gym)\n",
        "print(df_account_value_ddpg)\n",
        "print(df_ddpg_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp1RlBdzjLZE"
      },
      "source": [
        "The actions DataFrame shows that the DDPG model tends to buy a certain amount of assets, and hold them until the end of the trade. The same behaviour has noted when the TD3 model is tested, which imposes the conclusion that this is a common behaviour for these models, probably related to their embedded risk management, and the learning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSEuVX7gjLZF"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame with portfolio values in $\n",
        "df_account_value_ddpg = pd.DataFrame({'date': df_account_value_ddpg['date'], 'account_value_ddpg': df_account_value_ddpg['account_value']})\n",
        "df_account_value_ddpg = df_account_value_ddpg.set_index('date')\n",
        "df_account_value_ddpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1vOJ8-TjLZF"
      },
      "outputs": [],
      "source": [
        "# Calculate daily returns\n",
        "df_account_value_ddpg['daily_return_ddpg'] = df_account_value_ddpg['account_value_ddpg'].pct_change()\n",
        "\n",
        "# Calculate cumluative returns\n",
        "df_account_value_ddpg['cumuluative_return_ddpg'] = np.exp(np.log1p(df_account_value_ddpg['daily_return_ddpg']).cumsum())-1\n",
        "df_account_value_ddpg['cumuluative_return_ddpg']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfp0xw0_jLZF"
      },
      "source": [
        "## The Ensemble Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-gthCxMtj1d"
      },
      "outputs": [],
      "source": [
        "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
        "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
        "\n",
        "TRAIN_START_DATE = \"2021-01-01\"\n",
        "TRAIN_END_DATE = \"2024-03-01\"\n",
        "TEST_START_DATE = \"2024-03-02\"\n",
        "TEST_END_DATE = \"2025-12-31\"\n",
        "\n",
        "ensemble_agent = DRLEnsembleAgent(df=data,\n",
        "                                  train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
        "                                  val_test_period=(TEST_START_DATE,TEST_END_DATE),\n",
        "                                  rebalance_window=rebalance_window,\n",
        "                                  validation_window=validation_window,\n",
        "                                  **env_kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1lyCECstj1e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
        "                                               PPO_model_kwargs,\n",
        "                                               DDPG_model_kwargs,\n",
        "                                               timesteps_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0qd8acMtj1f"
      },
      "outputs": [],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6vvNSC6h1jZ"
      },
      "source": [
        "# The Ensemble Strategy Backtest\n",
        "To evaluate the performance of a trading strategy we use backtesting. There are automated backtesting tools, such as the Quantopian pyfolio package, and usually the automated testing is preferred because it reduces the posibillity of a human error. They are also easier to use, and contain various individual plots that provide a comprehensive image of the performance of a trading strategy. In this project backtesting was done 'manually' as the results were used for comparison with the other strategies.\n",
        "First, we will set the dates to use for backtesting a strategy on the trading period:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3nQx-KLjLZH"
      },
      "outputs": [],
      "source": [
        "unique_trade_date = data[(data['date'] > TEST_START_DATE)&(data['date'] <= TEST_END_DATE)]['date'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4JKB--8tj1g"
      },
      "outputs": [],
      "source": [
        "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
        "\n",
        "# Calculating the out-of-sample results\n",
        "df_account_value_ensemble = pd.DataFrame()\n",
        "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1, rebalance_window):\n",
        "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble', i))\n",
        "    df_account_value_ensemble = pd.concat([df_account_value_ensemble, temp], ignore_index=True)\n",
        "\n",
        "df_account_value_ensemble = df_account_value_ensemble.join(df_trade_date[validation_window:].reset_index(drop=True))\n",
        "df_account_value_ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HggausPRoCem",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create DataFrame with results from the Ensemble strategy\n",
        "\n",
        "df_account_value_ensemble = pd.DataFrame({'date': df_account_value_ensemble['date'], 'account_value_ensemble': df_account_value_ensemble['account_value']})\n",
        "df_account_value_ensemble = df_account_value_ensemble.set_index('date')\n",
        "df_account_value_ensemble.to_csv(\"ensemble_result.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR4_dLckjLZJ"
      },
      "outputs": [],
      "source": [
        "# Calculate daily returns\n",
        "df_account_value_ensemble['daily_return_ensemble'] = df_account_value_ensemble['account_value_ensemble'].pct_change()\n",
        "\n",
        "# Calculate cumluative returns\n",
        "df_account_value_ensemble['cumuluative_return_ensemble'] = np.exp(np.log1p(df_account_value_ensemble['daily_return_ensemble']).cumsum())-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmDko5iejLZJ"
      },
      "source": [
        "Parallel testing of strategies, with DDPG and TD3, resulted with TD3 perform much better as a standalone model, and DDPG usually having the worst standalone performance. However, in the Ensemble strategy, TD3 was rarely chosen into the ensemble trading strategy for the next quarter, resulting with lower performance of the Ensemble strategy, than using the DDPG model, because it used mostly A2C and PPO models, which showed us that more agents should be included. Furthermore, the unpredictable behaviour of standalone models using the same data and same hyperparameters, made them sometimes overperform the ensemble strategy, but in other cases with the same setting (same parameters and environment) have suboptimal outcome. In the case of the A2C algorithm, this is probably because of the liability of the A2C algorithm toward overfitting, which would make it a bad choice to use alone in the real-world crypto trading. Therefore, the Ensemble strategy is a good strategy to combine the three models utilising the best properties of each DRL algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-FjnysQjLZK"
      },
      "source": [
        "## Minimum Variance Optimisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fkGbSTsjLZK"
      },
      "source": [
        "The Minimum Variance Portfolio is the portfolio of assets that has the lowest possible variance (risk) for a given set of assets, without necessarily considering expected returns. This approach is purely focused on minimizing portfolio volatility, making it useful in risk-sensitive strategies or as a benchmark for comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkK6WXZejLZK"
      },
      "source": [
        "Train data is adjusted based on the step in the ensemble algorithm validation.\n",
        "The objective of the minimum variance optimization is to find the portfolio weights $\\mathbf{w} \\in \\mathbb{R}^n$ that minimize the portfolio variance:\n",
        "$$\n",
        "\\sigma_p^2 = \\mathbf{w}^\\top \\Sigma \\mathbf{w}\n",
        "$$\n",
        "\n",
        "Subject to:\n",
        "$$\n",
        "\\mathbf{1}^\\top \\mathbf{w} = 1\n",
        "$$\n",
        "$$\n",
        "-1 \\leq w_i \\leq 1 \\quad \\forall i\n",
        "$$\n",
        "\n",
        "Where $\\Sigma$ is the covariance matrix of asset returns, $\\mathbf{w}$ is the weight vector, $\\mathbf{1}$ is a vector of ones.\n",
        "\n",
        "This formulation allows for long/short positions, but doesn't limit the size of any position to 100% of the portfolio value, to prevent excessive leverage. As explained below, this hapenned in the case of Bitcoin in the portfolio, so in code, different constraints are used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQJuYiLbjLZL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates  # Dates module for x-axis formatting\n",
        "import cvxpy as cp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "e3dqdL-jjLZL"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('data.csv',parse_dates=['date'], index_col='date')\n",
        "data = data.sort_values(['date', 'tic'])\n",
        "data = data.pivot(columns='tic', values='close')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5gR6JqvjLZM"
      },
      "outputs": [],
      "source": [
        "# Splitting the data into the training and trading periods\n",
        "train_start = '2021-01-01'\n",
        "train_end = '2024-03-01'\n",
        "trade_start = '2024-05-05'\n",
        "trade_end = '2025-03-15'\n",
        "\n",
        "\n",
        "# Creating the train and trade datasets\n",
        "train = data[train_start:train_end]\n",
        "trade = data[trade_start:trade_end]\n",
        "\n",
        "# Calculating the covariance matrix from the training data returns\n",
        "returns = train.pct_change().dropna()\n",
        "cov_matrix = returns.cov()\n",
        "cov_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfGZgdHajLZM"
      },
      "source": [
        "The covariance matrix $\\Sigma \\in \\mathbb{R}^{n \\times n}$ captures how pairs of asset returns move together. Each element $\\Sigma_{ij}$ is defined as:\n",
        "\n",
        "$$\n",
        "\\Sigma_{ij} = \\text{Cov}(R_i, R_j)\n",
        "$$\n",
        "\n",
        "where the diagonal entries $\\Sigma_{ii}$ represent the **variance** of asset $i$. The off-diagonal entries $\\Sigma_{ij}$ for $i \\neq j$ represent the **covariance** (co-movement) between asset $i$ and asset $j$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD7ji-gWjLZM"
      },
      "outputs": [],
      "source": [
        "# Implementing the Minimum Variance Portfolio Optimization\n",
        "\n",
        "n = len(cov_matrix)\n",
        "weights = cp.Variable(n)\n",
        "portfolio_variance = cp.quad_form(weights, cov_matrix)\n",
        "\n",
        "# Setting the optimization constraints\n",
        "\n",
        "constraints = [cp.sum(weights) == 1, weights >= -1, weights <= 0.2]\n",
        "problem = cp.Problem(cp.Minimize(portfolio_variance), constraints)\n",
        "problem.solve()\n",
        "\n",
        "# Get the optimized weights\n",
        "\n",
        "optimal_weights = weights.value\n",
        "optimal_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SipvV5FcjLZN"
      },
      "source": [
        "Initially the optimization constraints for weights were the following: weights sum is equal to 1 as the minimum variance problem requires, and can be between -1 and 1, to allow short selling.\n",
        "Howewer, the results with the weights constraints [-1, 1], were in this case giving the following weights: [ 0.059111    1.  -0.01838817 -0.03523041 -0.00549243] using cvxpy, and the [ 0.06061209  1. -0.01816012 -0.03685465 -0.00559731] when using the same constraing and the scipy minimize method. To prevent the optimizer from allocating 100% to BTC, and ensure the diversity of the portfolio, the weights constraints [-1, 0.2] are used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAxuBODbjLZN"
      },
      "outputs": [],
      "source": [
        "# Calculate account value during the trading period\n",
        "initial_portfolio_value = 1000000\n",
        "\n",
        "# Get latest prices (most recent row of price DataFrame)\n",
        "last_prices = train.tail(1).to_numpy().flatten()\n",
        "min_weights = initial_portfolio_value * optimal_weights\n",
        "\n",
        "# Convert dollar allocation to number of shares\n",
        "initial_portfolio = min_weights / last_prices\n",
        "\n",
        "# Multiply asset quantities by daily prices to get portfolio value over time\n",
        "account_value_min_variance = trade @ initial_portfolio\n",
        "\n",
        "# Store the results in a DataFrame\n",
        "df_account_value_min_variance = pd.DataFrame(account_value_min_variance, columns=[\"account_value_min_variance\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDaJXrsnjLZN"
      },
      "outputs": [],
      "source": [
        "# Calculate daily returns\n",
        "portfolio_returns = account_value_min_variance.pct_change().dropna()\n",
        "\n",
        "# Calculate cumluative returns\n",
        "cumulative_returns = np.exp(np.log1p(portfolio_returns).cumsum())-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKyaKO7WjLZO"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame with results from the Minimum Variance strategy\n",
        "df_account_value_min_variance = pd.DataFrame({'account_value_min_variance': account_value_min_variance, 'cumuluative_return_min_variance': cumulative_returns}, index=trade.index)\n",
        "df_account_value_min_variance['cumuluative_return_min_variance'] = cumulative_returns\n",
        "df_account_value_min_variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ-BiHn9jLZO"
      },
      "source": [
        "## Creating data for performance comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HOLTNe7jLZP"
      },
      "outputs": [],
      "source": [
        "df_account_value_ensemble = df_account_value_ensemble.set_index(trade.index)\n",
        "data_frames = [df_account_value_ensemble, df_account_value_min_variance, df_account_value_a2c, df_account_value_ppo, df_account_value_ddpg]\n",
        "df_results = pd.concat(data_frames, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M-DWu9mjLZP"
      },
      "source": [
        "For creating perfomance analytics the empyrical library, from Quantopian, is used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmWELYZZjLZP"
      },
      "outputs": [],
      "source": [
        "cum_returns_min_var = round(cum_returns(portfolio_returns)*100, 2)\n",
        "sharpe_ratio_min_var = round(sharpe_ratio(portfolio_returns, risk_free=0, period='daily'), 2)\n",
        "max_drawdawn_min_var = round(max_drawdown(portfolio_returns)*100, 2)\n",
        "annual_return_min_var = round(annual_return(portfolio_returns, period='daily')*100, 2)\n",
        "annual_volatility_min_var = round(annual_volatility(portfolio_returns, period='daily')*100, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC0hOJchjLZQ"
      },
      "outputs": [],
      "source": [
        "cum_returns_ddpg = round(cum_returns(df_results['daily_return_ddpg'])*100, 2)\n",
        "sharpe_ratio_ddpg = round(sharpe_ratio(df_results['daily_return_ddpg'], risk_free=0, period='daily'), 2)\n",
        "max_drawdawn_ddpg = round(max_drawdown(df_results['daily_return_ddpg'])*100, 2)\n",
        "annual_return_ddpg = round(annual_return(df_results['daily_return_ddpg'], period='daily')*100, 2)\n",
        "annual_volatility_ddpg = round(annual_volatility(df_results['daily_return_ddpg'], period='daily')*100, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jwa0eCDHjLZQ"
      },
      "outputs": [],
      "source": [
        "cum_returns_a2c = round(cum_returns(df_results['daily_return_a2c'])*100, 2)\n",
        "sharpe_ratio_a2c = round(sharpe_ratio(df_results['daily_return_a2c'], risk_free=0, period='daily'), 2)\n",
        "max_drawdawn_a2c = round(max_drawdown(df_results['daily_return_a2c'])*100, 2)\n",
        "annual_return_a2c = round(annual_return(df_results['daily_return_a2c'], period='daily')*100, 2)\n",
        "annual_volatility_a2c = round(annual_volatility(df_results['daily_return_a2c'], period='daily')*100, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Wky1t-CjLZQ"
      },
      "outputs": [],
      "source": [
        "cum_returns_ppo = round(cum_returns(df_results['daily_return_ppo'])*100, 2)\n",
        "sharpe_ratio_ppo = round(sharpe_ratio(df_results['daily_return_ppo'], risk_free=0, period='daily'), 2)\n",
        "max_drawdawn_ppo = round(max_drawdown(df_results['daily_return_ppo'])*100, 2)\n",
        "annual_return_ppo = round(annual_return(df_results['daily_return_ppo'], period='daily')*100, 2)\n",
        "annual_volatility_ppo = round(annual_volatility(df_results['daily_return_ppo'], period='daily')*100, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYdosJ7qjLZR"
      },
      "outputs": [],
      "source": [
        "cum_returns_ensemble = round(cum_returns(df_results['daily_return_ensemble'])*100, 2)\n",
        "sharpe_ratio_ensemble = round(sharpe_ratio(df_results['daily_return_ensemble'], risk_free=0, period='daily'), 2)\n",
        "max_drawdawn_ensemble = round(max_drawdown(df_results['daily_return_ensemble'])*100, 2)\n",
        "annual_return_ensemble = round(annual_return(df_results['daily_return_ensemble'], period='daily')*100, 2)\n",
        "annual_volatility_ensemble = round(annual_volatility(df_results['daily_return_ensemble'], period='daily')*100, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1utDmPpjLZR"
      },
      "source": [
        "## Plotting the final results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgokzGnQjLZR"
      },
      "outputs": [],
      "source": [
        "#Plotting the comparison of the portfolio values in the dollar value\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(df_results.index, df_results['account_value_min_variance'], label='Minimum Variance', color='orange')\n",
        "plt.plot(df_results.index, df_results['account_value_ensemble'], label='Ensemble Strategy', color='blue')\n",
        "plt.plot(df_results.index, df_results['account_value_a2c'], label='A2C', color='maroon')\n",
        "plt.plot(df_results.index, df_results['account_value_ppo'], label='PPO', color='purple')\n",
        "plt.plot(df_results.index, df_results['account_value_ddpg'], label='DDPG', color='green')\n",
        "\n",
        "# Setting the title and labels\n",
        "plt.title('Portfolio Values')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Portfolio Values')\n",
        "plt.axhline(y=initial_portfolio_value, color='r', linestyle='--', label='Initial Portfolio Value')\n",
        "\n",
        "# Formatting the x-axis to show months\n",
        "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())  # Set major ticks to months\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))  # Format the date display\n",
        "\n",
        "# Rotating the date labels\n",
        "plt.gcf().autofmt_xdate()\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md1SzSP3jLZS"
      },
      "outputs": [],
      "source": [
        "# Plotting the comparison of cumulative returns\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(df_results.index, df_results['cumuluative_return_min_variance'], label='Minimum Variance', color='orange')\n",
        "plt.plot(df_results.index, df_results['cumuluative_return_ensemble'], label='Ensemble Strategy', color='blue')\n",
        "plt.plot(df_results.index, df_results['cumuluative_return_a2c'], label='A2C', color='yellow')\n",
        "plt.plot(df_results.index, df_results['cumuluative_return_ppo'], label='PPO', color='purple')\n",
        "plt.plot(df_results.index, df_results['cumuluative_return_ddpg'], label='DDPG', color='green')\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('Cumulative Returns')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Returns')\n",
        "#plt.axhline(y=initial_portfolio_value, color='r', linestyle='--', label='Initial Portfolio Value')\n",
        "\n",
        "# Formatting the x-axis to show months\n",
        "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())  # Set major ticks to months\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))  # Format the date display\n",
        "\n",
        "# Rotate date labels\n",
        "plt.gcf().autofmt_xdate()\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY2rnGfnjLZS"
      },
      "source": [
        "Parallel testing of strategies, with DDPG and TD3, resulted with TD3 perform much better as a standalone model, and DDPG usually having the worst standalone performance. However, in the Ensemble strategy, when the TD3 model was used the Ensemble strategy had the lower performance, than when using the DDPG model.\n",
        "\n",
        "Furthermore, the unstable behaviour of standalone models (specifically A2C and DDPG, where PPO showed to be the most stable) using the same data and same hyperparameters, made them sometimes overperform the Ensemble strategy, but in other cases with the same setting (same parameters and environment) have suboptimal outcome.\n",
        "\n",
        "The number of timesteps for agents is one the parameters that has the biggest influence on how the agents behave and perform.\n",
        "Also, all models and strategies have low performance when prices are declining, which can be seen on a plot in April 2025., when all five cryptocurrencies had a significant price drop. Similarily, in December 2024., all five cryptocurrencies had a significant surge in prices which reflected on the agents and strategies performance.\n",
        "\n",
        "The most capable in following trends and volatile market, even with the small number of timesteps (tested on 5000 timesteps) is the PPO model, where it outperformed all other strategies, as it can explore early, and then exploit policies with clipped updates.\n",
        "\n",
        "The A2C model shows fastest reactions to price changes, but doesn't have a good performance in a bear market, thereforet it would be a good choice in a flat market, or a stable or sideways markets. This behaviour is probably because of the liability of the A2C algorithm toward overfitting, which would make it a bad choice to use alone in the real-world crypto trading.\n",
        "\n",
        "The worst performance was shown by the DDPG agent, because of its buy/hold early strategy where its performance deprecates when the prices are in down.\n",
        "\n",
        "Good performance of a single agent does not guarantee the best performance of the Ensemble strategy. Therefore, the Ensemble strategy is a good strategy to combine the three models utilising the best properties of each DRL algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHDl0dSSjLZT"
      },
      "source": [
        "## Comparison of the performance metrics in %\n",
        "\n",
        "### Sharpe Ratio\n",
        "\n",
        "The Sharpe Ratio is one of the most widely used metrics for evaluating the risk-adjusted return of an investment or trading strategy, including crypto portfolios. It helps investors understand how much excess return they are receiving for the extra volatility they endure from holding a riskier asset. The Sharpe Ratio quantifies how well a portfolio compensates for volatility. In crypto, where volatility is extreme, a good Sharpe ratio is harder to achieve but more meaningful than just a raw return.\n",
        "\n",
        "The Sharpe Ratio is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Sharpe Ratio} = \\frac{R_p - R_f}{\\sigma_p}\n",
        "$$\n",
        "\n",
        "where $R_p$ is the portfolio return, $R_f$ is the risk-free rate, and $\\sigma_p$  is the standard deviation of portfolio returns.\n",
        "A higher Sharpe ratio indicates better risk-adjusted performance, while a negative Sharpe ratio means that taking the risk but not being compensated.\n",
        "\n",
        "### Cumulative Return\n",
        "\n",
        "The Cumulative Return measures the total return of an investment over a period of time, taking into account compounding. It is calculated using the product of daily returns.\n",
        "The cumulative return is defined as:\n",
        "$$\n",
        "\\text{Cumulative Return} = \\prod_{t=1}^{T} (1 + R_t) - 1\n",
        "$$\n",
        "where $R_t$ is the return at time $t$, and $T$ is the total number of time periods.\n",
        "\n",
        "For example, a portfolio had the following daily returns over 3 days:\n",
        "Day 1: +2% (0.02), Day 2: -1% (-0.01), Day 3: +1.5% (0.015)\n",
        "\n",
        "Then:\n",
        "$$\n",
        "\\text{Cumulative Return} = (1+0.02)(1−0.01)(1+0.015) − 1 =1.02×0.99×1.015 − 1 \\approx 0.0241\n",
        "$$\n",
        "### Annual Return\n",
        "\n",
        "Annual Return, also referred to as Annualized Return, is a standardized metric that expresses the rate of return of an investment on a yearly basis, regardless of the actual time frame of the investment. It represents the geometric average of returns achieved over a given period, scaled to one year. This metric is particularly useful in financial performance analysis, as it allows investors, analysts, and researchers to compare the profitability of assets or strategies that span different time durations.\n",
        "\n",
        "For example, if one crypto trading strategy generates a 30% return over 18 months, and another yields a 25% return over 12 months, their raw returns are not directly comparable. Annualizing both returns provides a common temporal basis to assess which strategy has been more effective per unit time.\n",
        "\n",
        "The annualized return incorporates the effect of compounding, making it a more accurate reflection of investment growth than simply dividing the total return by the number of years. It is a critical performance indicator in evaluating long-term trading strategies, portfolio management, and financial models like reinforcement learning agents in algorithmic trading.\n",
        "\n",
        "The annual return is defined as:  \n",
        "$$\n",
        "\\text{Annual Return} = (1 + R_{\\text{total}})^{\\frac{1}{n}} - 1\n",
        "$$  \n",
        "where $R_{\\text{total}}$ is the cumulative return over the full period, and $n$ is the number of years.\n",
        "\n",
        "For example, if a portfolio achieves a cumulative return of 25% over 1.5 years, the annualized return is  \n",
        "$$\n",
        "\\text{Annual Return} = (1 + 0.25)^{\\frac{1}{1.5}} - 1 \\approx 0.157\n",
        "$$.\n",
        "\n",
        "### Annual Volatility\n",
        "\n",
        "The Annual Volatility is a measure of the degree of variation in daily (or other period) returns of an asset or a portfolio, scaled to a one-year time frame. It reflects how much the returns fluctuate around the mean, and is often used as a proxy for risk. In the context of crypto, which is highly volatile, annualized volatility gives a meaningful way to assess and compare the riskiness of different coins or strategies.\n",
        "\n",
        "Annual volatility is defined as:  \n",
        "$$\n",
        "\\text{Annual Volatility} = \\sigma_d \\times \\sqrt{N}\n",
        "$$  \n",
        "where $\\sigma_d$ is the standard deviation of daily returns, and $N$ is the number of trading days in a year (usually $N = 252$).\n",
        "\n",
        "### Maximum Drawdown\n",
        "\n",
        "A Maximum Drawdown (MDD) is a risk metric that measures the largest peak-to-trough decline in a portfolio’s value over a specific time period. It shows the worst-case loss an investor might have experienced if they bought an asset at the peak price and sold at the lowest price.\n",
        "\n",
        "A MMD is an important metric for financial trading and reinforcement learning in trading environments, because it reflects a downside risk, which is especially relevant in highly volatile markets like cryptocurrencies.\n",
        "\n",
        "Maximum Drawdown (MDD) is defined as:\n",
        "\n",
        "$$\n",
        "\\text{MDD} = \\max_{t \\in [0,T]} \\left( \\frac{\\max_{\\tau \\in [0,t]} P_\\tau - P_t}{\\max_{\\tau \\in [0,t]} P_\\tau} \\right)\n",
        "$$\n",
        "\n",
        "where $P_t$ is the portfolio value at time $t$, and $\\max_{\\tau \\in [0,t]}$ $P_\\tau$  is the historical peak before or at time $t$.\n",
        "The outer $max$ operator finds the worst (maximum) drawdown over the time interval $[0,T]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfPzSGujjLZT"
      },
      "outputs": [],
      "source": [
        "# Creating an empty DataFrame with columns and row names for\n",
        "df_performance = pd.DataFrame(columns = ['Enseble', 'Min-Var', 'A2C', 'PPO', 'DDPG'],\n",
        "                  index = ['Sharpe Ratio', 'Cumulative Return', 'Annual Return', 'Annual Volatility', 'Maximum Drawdown'])\n",
        "\n",
        "# Adding rows to an empty dataframe at existing index\n",
        "df_performance.loc['Sharpe Ratio'] = [sharpe_ratio_ensemble, sharpe_ratio_min_var, sharpe_ratio_a2c, sharpe_ratio_ppo, sharpe_ratio_ddpg]\n",
        "df_performance.loc['Cumulative Return'] = [cum_returns_ensemble[-1], cum_returns_min_var[-1], cum_returns_a2c[-1], cum_returns_ppo[-1], cum_returns_ddpg[-1]]\n",
        "df_performance.loc['Annual Return'] = [annual_return_ensemble, annual_return_min_var, annual_return_a2c, annual_return_ppo, annual_return_ddpg]\n",
        "df_performance.loc['Annual Volatility'] = [annual_volatility_ensemble, annual_volatility_min_var, annual_volatility_a2c, annual_volatility_ppo, annual_volatility_ddpg]\n",
        "df_performance.loc['Maximum Drawdown'] = [max_drawdawn_ensemble, max_drawdawn_min_var, max_drawdawn_a2c, max_drawdawn_ppo, max_drawdawn_ddpg]\n",
        "df_performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI3Rtm2ZjLZU"
      },
      "source": [
        "From the performance table we can see that the Ensemble strategy performed the best in all categories. As mentioned before, from the standalone DRL agents, the PPO showed the best performance in generating returns and following trends and performes the best in both bullish and bearish markets, with the lowest maximum drowdown, under these parameters.\n",
        "\n",
        "The DDPG agent has a similar, but inferior performance as the PPO, and can be used a good addition to the Ensemble strategy, but not as a standalone agent as it performs worse than the Minimum Variance benchhmark.\n",
        "\n",
        "The A2C in this simulation had the worst performance, but it changes in repetitive simulations, because of the instability of lerning. The A2C agent learns from the $n$ steps before every policy update, and drops the sample, taking a new sample in every timestep, meaning that the policy is fully based on the latest experience. This behaviour can lead to high-variance, unstable learning. The A2C agent may react excesive to short-term price changes. While A2C is a powerful reinforcement learning algorithm, it has several limitations when applied to financial trading. The A2C agent struggles to generalize across different market conditions, leading to poor performance when trends shift.\n",
        "\n",
        "The PPO agent collects a big batch of data (e.g. 2048 steps), and splits it into mini-batches (e.g., 128 samples each), meaning that the PPO agent uses the same batch 16 times, which results in more efficient learning. This is called mini-batch stochastic gradient descent, and it helps the agent a better use of each experience, the model generalize rather than just memorize, and avoids throwing away good data after one update (unlike A2C)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jYyJEwxjLZU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "citation-manager": {
      "items": {}
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}